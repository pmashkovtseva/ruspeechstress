всем привет! сейчас мы поговорим о задачах извлечения признаков. центральное положение занимают методы, основанные на машинном обучении (так уж сейчас сложилось). такие методы получают на вход специальные структуры данных — чаще всего это вектора или матрицы (тензоры). иногда алгоритмы принимают на вход графы или деревья. зачем рассматривать вообще задачу извлечения признаков отдельно от конечной задачи? на самом деле на это есть множество причин. первая заключается в том, что значительная часть методов извлечения признаков не требует разметки. следовательно, мы можем применить методы к большим неразмеченным корпусам, получить хорошие признаки, а потом уже работать с небольшой размеченной выборкой. с другой стороны, возможно, получится извлечь признаки один раз, а потом поверх них накручивать классификаторы для решения разных задач. а вообще бывает, что можно получить неплохое представление, не используя машинного обучения вообще. в этой лекции мы рассмотрим несколько самых популярных методов по мере их усложнения. для каждого подхода мы выделим основные преимущества и недостатки. в первую очередь, рассмотрим разреженное векторное представление, популярное в классических подходах, а затем перейдём к более сложным нейросетевым и ядерным подходам. это, в первую очередь — обзор, призванный расширить кругозор и дать правильную терминологию для дальнейшего самостоятельного изучения. простейший метод — это двоичный вектор. элементы вектора соответствуют отдельным словам. элемент равен "1", например вот здесь, если слово в документе присутствует, и "0", если нет. это очень простой метод. он подходит, в том числе, тогда, когда тексты сильно отличаются по длине. размерность векторного пространства, получаемого таким образом, достаточно большая, и поэтому почти любые классы линейно разделимы, и линейные модели хорошо работают на таких пространствах. однако главная проблема заключается в том, что и частотные слова общеупотребимые, и специальная лексика, имеют одинаковый вес, то есть мы не знаем, как много используется каждое слово в документе. по этой же причине метод чувствителен к опечаткам, случайным словам, и так далее. общая проблема разреженных векторных моделей — в так называемом "предположении о независимости". элементы вектора, соответствующие разным словам заполняются независимо, и таким образом мы теряем информацию о том, что какие-то два слова являются синонимами и если одно встретилось, то, возможно, может встретиться и другое. из-за этого модель может не очень хорошо обобщаться на новые данные. и это происходит не из-за переобучения, а из-за того, что просто признаки не очень хорошие, в них просто нет такой информации. в то время как высокая размерность вектора может рассматриваться как преимущество, она может также являться и недостатком, потому что если обучающая выборка не очень большая, а признаков при таком подходе получается очень много — сотни тысяч, мы можем переобучиться даже если работаем с простой линейной моделью.